{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Navigation Project\n",
    "This notebook contains my solution for the navigation project. A UnityEnvironment [1] in which an agent should learn to collect yellow bananas, while avoiding blue banans, was provided by Udacity.\n",
    "\n",
    "The agent uses the DQN network (Mnith et al., 2015, [2]) and two improvements, namely Double-DQN (van Hasselt et al., 2015, [3]) and Dueling Network Architectures ( Wang et al., 2015, [4]).\n",
    "\n",
    "I emphasized simplicty in the implementation of replay memory and agent over the most efficient or most expandability. Before writing my implementation I studied the DQN example from the Udacity Deep Reinforcement Learning Repository [5], the instructions for the navigation project [6] and the DQN implementation from OpenAi Baselines [7]. Additionally, the PyTorch documentation & tutorial page was very helpful to get started on PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning algorithm\n",
    "\n",
    "The agent uses the DQN algorithm by Mnith et al. (2015, [2]), in which a neural network is used in order to learn the Q-values of an environment. \n",
    "At time $t$, the agent is in state $s_t$ and chooses an action $a_t$ with an $\\epsilon$ greedy policy and observes the response of the environment (especially the reward $r_{t+1}$ and the next state $s_{t+1}$).\n",
    "The tuple ($s_t$, $a_t$, $r_{t+1}$, $s_{t+1}$) forms on element of experience and is saved in the Replay Memory.\n",
    "When enough experience is collected, the agent samples from the Replay Memory randomly and uses this vector of experience for a batch update of the q networks.\n",
    "To update the network, we try to minimize the loss [2]:\n",
    "$$ L = r_{t+1} + \\gamma * max_{a} Q(s_{t+1}, a, \\theta') - Q(s_{t}, a_{t}, \\theta ) $$,\n",
    "where $\\theta$ is the Q-network and $\\theta'$ is the target network. The target network is updated to be a copy of the q-network after n steps. \n",
    "\n",
    "The Double-DQN algorithm [3] sightly modifies the learning step for the q-network in order to obtain a more stable update. In the dueling network method [4], the neural network used to obtain the Q-values is split in one part representing the value of a state and the so-called \"advantage\", which represents the additional value coming from choosing a particular action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from collections import deque, namedtuple\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# Settings\n",
    "\n",
    "# Paths and folders:\n",
    "\n",
    "environment_path = \"/data/Banana_Linux_NoVis/Banana.x86_64\"\n",
    "model_save_path = \"./data/\"\n",
    "\n",
    "# Select the different versions of the agent that you want to train\n",
    "Setting = namedtuple(\"Settings\",'name, lr, ddqn, duel')\n",
    "test_settings = []\n",
    "test_settings.append(Setting(\"DQN\", 5e-4, False, False))   # Plain DQN\n",
    "test_settings.append(Setting(\"DDQN\", 5e-4, True, False))    # DDQN\n",
    "test_settings.append(Setting(\"DDQN + Duel\", 5e-4, True, True))     # DDQN + Duel Network\n",
    "\n",
    "# maximal number of episodes each agent is trained on\n",
    "number_episodes = 1000\n",
    "# Choose if your calculations are performed on a GPU or on the CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Experience = namedtuple('Experience','state action reward next_state done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "The dqn class is used to implement the neural network for the standard DQN algorithm as well as the dueling network algorithm. You can select the dueling network with the duel flag in the constructor.\n",
    "\n",
    "The standard DQN model has three fully-connected hidden layers with 64 neurons each and four outputs, one output for each possible action. After the first two hidden layers a ReLU is used as the activation function.\n",
    "\n",
    "The dueling network uses two layers with 64 neurons in the beginning, afterwards the network is split and one part for the state value and one part for the advantage. The state value part has one layer with 64 neurons and a second layer with 48 neurons. The advantage part uses slightly more neurons (64 in the first and 64 in the second layer), since it ultimately also has more outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn(nn.Module):\n",
    "    def __init__(self, state_size, action_size, duel = False):\n",
    "        super(dqn, self).__init__()\n",
    "        self.duel = duel\n",
    "        self.fc1 = nn.Linear( state_size, 64 )\n",
    "        self.fc2 = nn.Linear( 64, 64 )\n",
    "        self.fc3 = nn.Linear( 64, action_size )\n",
    "        \n",
    "        self.fc3v = nn.Linear( 64, 48)\n",
    "        self.fc4v = nn.Linear( 48, 1 )\n",
    "        self.fc3a = nn.Linear( 64, 64)\n",
    "        self.fc4a = nn.Linear( 64, action_size )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        if not self.duel:\n",
    "            x = self.fc3(x)               # The regular case\n",
    "        else:\n",
    "            v = F.relu(self.fc3v( x ))    # The duel network architecture\n",
    "            v = self.fc4v( v )\n",
    "            a = F.relu(self.fc3a( x ))\n",
    "            a = self.fc4a( a )\n",
    "            x = v + (a - a.mean())        # Subtract a.mean(), another choice would be a.max()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Replay Memory is used to save and sample experience. The deque can only cold maxLen elements, thus older elements are deleted from Replay Memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, maxLen ):\n",
    "        self.data = deque(maxlen=maxLen)\n",
    "        return\n",
    "    def add(self, sample):\n",
    "        self.data.append(sample)\n",
    "        return\n",
    "    def sample(self, number_samples):\n",
    "        length = len(self.data)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        if(length > number_samples):\n",
    "            elements = list(range(length))\n",
    "            indices = sample(elements, number_samples)\n",
    "            for index in indices:\n",
    "                state, action, reward, next_state, done = self.data[index]\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "        states_d = torch.from_numpy(np.array(states)).float().unsqueeze(1).to(device)\n",
    "        actions_d = torch.from_numpy(np.array(actions)).long().unsqueeze(1).to(device)\n",
    "        rewards_d = torch.from_numpy(np.array(rewards)).float().unsqueeze(1).to(device)\n",
    "        next_states_d = torch.from_numpy(np.array(next_states)).float().to(device)\n",
    "        dones_d = torch.from_numpy(np.array(dones).astype(np.float32)).float().unsqueeze(1).to(device)\n",
    "\n",
    "        # Returning experience vectors, similar to the DQN network solution from [5]\n",
    "        return tuple((states_d, actions_d, rewards_d, next_states_d, dones_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the implementation of the agent, there are various hyperparameters that can be set.\n",
    "The parameters are explained in the comments of the __init__ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "   class Agent():\n",
    "    def __init__(self, state_size, action_size, settings, maxLen = 10000):\n",
    "        \n",
    "        # Create the replay memory and the two neural networks and the optimizer function\n",
    "        self.mem = ReplayMemory(maxLen)\n",
    "        self.q_net = dqn(state_size, action_size).to(device)\n",
    "        self.tgt_net = dqn(state_size, action_size).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr = settings.lr)\n",
    "        \n",
    "        # Set some constants and hyperparameters for the agent\n",
    "        self.duel = settings.duel                 # Using duel network architecture or not\n",
    "        self.ddqn = settings.ddqn                 # Using the DDQN update step instead of DQN\n",
    "        self.epsilon_start = 1                    # Epsilon governs the amount of exploration in the action selection step\n",
    "        self.epsilon_decay_rate = 1.0/100000.0    # Epsilon should decay in order to have more exploitation later in training.\n",
    "        self.epsilon_min = 0.01                   # The decay is governed by the epsilon_decay_rate and epsilon_min\n",
    "        self.update_every = 4                     # Update the q-network after update_every steps\n",
    "        self.copy_net_every = 1000                # How often is tgt_net updated\n",
    "        self.batch_size = 64                      # Size of the batch used for training\n",
    "        self.gamma = 0.99                         # Decay rate for future updates\n",
    "\n",
    "        #reset some variables\n",
    "        self.count = 0\n",
    "        self.epsilon = self.epsilon_start\n",
    "\n",
    "    def update(self, sample):\n",
    "        self.count += 1\n",
    "        self.epsilon = max(self.epsilon - self.epsilon, self.epsilon_min)\n",
    "        self.mem.add(sample)\n",
    "        \n",
    "        if self.count%self.copy_net_every == 0:\n",
    "            self.tgt_net.load_state_dict(self.q_net.state_dict())\n",
    "        if self.count > self.batch_size and self.count%self.update_every == 0:\n",
    "            states, actions, rewards, next_states, dones = self.mem.sample(self.batch_size)\n",
    "            \n",
    "            if not self.ddqn:\n",
    "                q_target = rewards + self.gamma * (1.0 - dones) * self.tgt_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            else:\n",
    "                next_action = self.q_net(next_states).max(1)[1].unsqueeze(1)\n",
    "                next_q = self.tgt_net(next_states).squeeze(1).gather(1, next_action)\n",
    "                q_target = rewards + self.gamma * (1.0 - dones) * next_q\n",
    "\n",
    "            q_target = q_target.detach()\n",
    "            q_prediction = self.q_net(states).squeeze(1)\n",
    "            q_prediction = q_prediction.gather(1, actions)\n",
    "            loss = F.mse_loss(q_target, q_prediction)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if(np.random.rand() < self.epsilon):\n",
    "            return np.argmax(np.random.rand(4))\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "            qvals = self.q_net(state).cpu().data.numpy()\n",
    "            return np.argmax(qvals)\n",
    "    def save_nns(self, path):\n",
    "        q_net_path = model_save_path + str(setting) + \"_q_net.pt\"\n",
    "        torch.save(self.q_net.state_dict(), q_net_path)\n",
    "        tgt_net_path = model_save_path + str(setting) + \"_tgt_net.pt\"\n",
    "        torch.save(self.tgt_net.state_dict(), tgt_net_path)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting\n",
      "Playing episode 0 reward: 0.0 average reward: 0.0\n",
      "Playing episode 5 reward: 1.0 average reward: 0.666666666667\n",
      "Playing episode 10 reward: 0.0 average reward: 0.363636363636\n",
      "Playing episode 15 reward: 1.0 average reward: 0.25\n",
      "Playing episode 20 reward: 3.0 average reward: 0.571428571429\n",
      "Playing episode 25 reward: -2.0 average reward: 0.730769230769\n",
      "Playing episode 30 reward: 4.0 average reward: 0.967741935484\n",
      "Playing episode 35 reward: 0.0 average reward: 1.22222222222\n",
      "Playing episode 40 reward: 4.0 average reward: 1.78048780488\n",
      "Playing episode 45 reward: 4.0 average reward: 2.30434782609\n",
      "Playing episode 50 reward: 6.0 average reward: 2.80392156863\n",
      "Playing episode 55 reward: 10.0 average reward: 3.21428571429\n",
      "Playing episode 60 reward: 15.0 average reward: 3.91803278689\n",
      "Playing episode 65 reward: 14.0 average reward: 4.51515151515\n",
      "Playing episode 70 reward: 9.0 average reward: 4.88732394366\n",
      "Playing episode 75 reward: 15.0 average reward: 5.26315789474\n",
      "Playing episode 80 reward: 11.0 average reward: 5.2962962963\n",
      "Playing episode 85 reward: 9.0 average reward: 5.51162790698\n",
      "Playing episode 90 reward: 12.0 average reward: 5.85714285714\n",
      "Playing episode 95 reward: 11.0 average reward: 6.23958333333\n",
      "Playing episode 100 reward: 12.0 average reward: 6.52\n",
      "Playing episode 105 reward: 10.0 average reward: 6.93\n",
      "Playing episode 110 reward: 8.0 average reward: 7.45\n",
      "Playing episode 115 reward: 13.0 average reward: 7.92\n",
      "Playing episode 120 reward: 10.0 average reward: 8.39\n",
      "Playing episode 125 reward: 9.0 average reward: 9.12\n",
      "Playing episode 130 reward: 7.0 average reward: 9.46\n",
      "Playing episode 135 reward: 10.0 average reward: 9.77\n",
      "Playing episode 140 reward: 13.0 average reward: 10.13\n",
      "Playing episode 145 reward: 12.0 average reward: 10.49\n",
      "Playing episode 150 reward: 10.0 average reward: 10.71\n",
      "Playing episode 155 reward: 17.0 average reward: 11.01\n",
      "Playing episode 160 reward: 13.0 average reward: 11.05\n",
      "Playing episode 165 reward: 14.0 average reward: 11.05\n",
      "Playing episode 170 reward: 3.0 average reward: 11.18\n",
      "Playing episode 175 reward: 18.0 average reward: 11.24\n",
      "Playing episode 180 reward: 13.0 average reward: 11.53\n",
      "Playing episode 185 reward: 10.0 average reward: 11.62\n",
      "Playing episode 190 reward: 18.0 average reward: 11.71\n",
      "Playing episode 195 reward: 18.0 average reward: 11.6\n",
      "Playing episode 200 reward: 7.0 average reward: 11.61\n",
      "Playing episode 205 reward: 11.0 average reward: 11.71\n",
      "Playing episode 210 reward: 10.0 average reward: 11.74\n",
      "Playing episode 215 reward: 3.0 average reward: 11.7\n",
      "Playing episode 220 reward: 9.0 average reward: 11.79\n",
      "Playing episode 225 reward: 15.0 average reward: 11.63\n",
      "Playing episode 230 reward: 11.0 average reward: 11.53\n",
      "Playing episode 235 reward: 8.0 average reward: 11.53\n",
      "Playing episode 240 reward: 11.0 average reward: 11.3\n",
      "Playing episode 245 reward: 14.0 average reward: 11.18\n",
      "Playing episode 250 reward: 8.0 average reward: 11.19\n",
      "Playing episode 255 reward: 19.0 average reward: 11.1\n",
      "Playing episode 260 reward: 17.0 average reward: 11.17\n",
      "Playing episode 265 reward: 11.0 average reward: 11.17\n",
      "Playing episode 270 reward: 13.0 average reward: 11.29\n",
      "Playing episode 275 reward: 17.0 average reward: 11.41\n",
      "Playing episode 280 reward: 11.0 average reward: 11.45\n",
      "Playing episode 285 reward: 19.0 average reward: 11.66\n",
      "Playing episode 290 reward: 10.0 average reward: 11.56\n",
      "Playing episode 295 reward: 15.0 average reward: 11.59\n",
      "Playing episode 300 reward: 14.0 average reward: 11.62\n",
      "Playing episode 305 reward: 2.0 average reward: 11.55\n",
      "Playing episode 310 reward: 14.0 average reward: 11.66\n",
      "Playing episode 315 reward: 20.0 average reward: 11.76\n",
      "Playing episode 320 reward: 10.0 average reward: 11.72\n",
      "Playing episode 325 reward: 4.0 average reward: 11.83\n",
      "Playing episode 330 reward: 13.0 average reward: 12.12\n",
      "Playing episode 335 reward: 8.0 average reward: 12.23\n",
      "Playing episode 340 reward: 14.0 average reward: 12.49\n",
      "Playing episode 345 reward: 15.0 average reward: 12.61\n",
      "Playing episode 350 reward: 10.0 average reward: 12.63\n",
      "Playing episode 355 reward: 9.0 average reward: 12.61\n",
      "Playing episode 360 reward: 12.0 average reward: 12.52\n",
      "Playing episode 365 reward: 12.0 average reward: 12.53\n",
      "Playing episode 370 reward: 14.0 average reward: 12.51\n",
      "Playing episode 375 reward: 12.0 average reward: 12.4\n",
      "Playing episode 380 reward: 18.0 average reward: 12.54\n",
      "Playing episode 385 reward: 11.0 average reward: 12.43\n",
      "Playing episode 390 reward: 16.0 average reward: 12.65\n",
      "Playing episode 395 reward: 15.0 average reward: 12.83\n",
      "Playing episode 400 reward: 18.0 average reward: 12.93\n",
      "Playing episode 405 reward: 12.0 average reward: 13.09\n",
      "Playing episode 410 reward: 13.0 average reward: 13.17\n",
      "Playing episode 415 reward: 9.0 average reward: 13.12\n",
      "Playing episode 420 reward: 18.0 average reward: 13.22\n",
      "Playing episode 425 reward: 19.0 average reward: 13.25\n",
      "Playing episode 430 reward: 14.0 average reward: 13.44\n",
      "Playing episode 435 reward: 9.0 average reward: 13.56\n",
      "Playing episode 440 reward: 15.0 average reward: 13.57\n",
      "Playing episode 445 reward: 14.0 average reward: 13.47\n",
      "Playing episode 450 reward: 16.0 average reward: 13.58\n",
      "Playing episode 455 reward: 16.0 average reward: 13.8\n",
      "Playing episode 460 reward: 15.0 average reward: 13.92\n",
      "Playing episode 465 reward: 12.0 average reward: 14.06\n",
      "Playing episode 470 reward: 12.0 average reward: 14.07\n",
      "Playing episode 475 reward: 19.0 average reward: 14.23\n",
      "Playing episode 480 reward: 12.0 average reward: 14.02\n",
      "Playing episode 485 reward: 21.0 average reward: 14.11\n",
      "Playing episode 490 reward: 14.0 average reward: 14.08\n",
      "Playing episode 495 reward: 8.0 average reward: 13.98\n",
      "Playing episode 500 reward: 20.0 average reward: 14.22\n",
      "Playing episode 505 reward: 11.0 average reward: 14.14\n",
      "Playing episode 510 reward: 16.0 average reward: 14.05\n",
      "Playing episode 515 reward: 13.0 average reward: 14.32\n",
      "Playing episode 520 reward: 15.0 average reward: 14.3\n",
      "Playing episode 525 reward: 17.0 average reward: 14.15\n",
      "Playing episode 530 reward: 11.0 average reward: 13.93\n",
      "Playing episode 535 reward: 18.0 average reward: 14.08\n",
      "Playing episode 540 reward: 11.0 average reward: 14.02\n",
      "Playing episode 545 reward: 9.0 average reward: 13.93\n",
      "Playing episode 550 reward: 12.0 average reward: 13.79\n",
      "Playing episode 555 reward: 12.0 average reward: 13.66\n",
      "Playing episode 560 reward: 11.0 average reward: 13.67\n",
      "Playing episode 565 reward: 5.0 average reward: 13.32\n",
      "Playing episode 570 reward: 9.0 average reward: 13.12\n",
      "Playing episode 575 reward: 7.0 average reward: 12.98\n",
      "Playing episode 580 reward: 16.0 average reward: 13.18\n",
      "Playing episode 585 reward: 11.0 average reward: 13.1\n",
      "Playing episode 590 reward: 9.0 average reward: 12.91\n",
      "Playing episode 595 reward: 7.0 average reward: 12.8\n",
      "Playing episode 600 reward: 15.0 average reward: 12.52\n",
      "Playing episode 605 reward: 10.0 average reward: 12.46\n",
      "Playing episode 610 reward: 12.0 average reward: 12.38\n",
      "Playing episode 615 reward: 10.0 average reward: 12.31\n",
      "Playing episode 620 reward: 18.0 average reward: 12.29\n",
      "Playing episode 625 reward: 15.0 average reward: 12.25\n",
      "Playing episode 630 reward: 8.0 average reward: 12.31\n",
      "Playing episode 635 reward: 17.0 average reward: 12.21\n",
      "Playing episode 640 reward: 4.0 average reward: 12.2\n",
      "Playing episode 645 reward: 7.0 average reward: 12.32\n",
      "Playing episode 650 reward: 9.0 average reward: 12.06\n",
      "Playing episode 655 reward: 16.0 average reward: 12.11\n",
      "Playing episode 660 reward: 17.0 average reward: 12.04\n",
      "Playing episode 665 reward: 12.0 average reward: 12.3\n",
      "Playing episode 670 reward: 15.0 average reward: 12.36\n",
      "Playing episode 675 reward: 7.0 average reward: 12.44\n",
      "Playing episode 680 reward: 15.0 average reward: 12.39\n",
      "Playing episode 685 reward: 15.0 average reward: 12.35\n",
      "Playing episode 690 reward: 15.0 average reward: 12.34\n",
      "Playing episode 695 reward: 12.0 average reward: 12.31\n",
      "Playing episode 700 reward: 14.0 average reward: 12.2\n",
      "Playing episode 705 reward: 19.0 average reward: 12.41\n",
      "Playing episode 710 reward: 13.0 average reward: 12.32\n",
      "Playing episode 715 reward: 18.0 average reward: 12.37\n",
      "Playing episode 720 reward: 8.0 average reward: 12.22\n",
      "Playing episode 725 reward: 9.0 average reward: 12.26\n",
      "Playing episode 730 reward: 13.0 average reward: 12.01\n",
      "Playing episode 735 reward: 11.0 average reward: 11.73\n",
      "Playing episode 740 reward: 9.0 average reward: 11.66\n",
      "Playing episode 745 reward: 10.0 average reward: 11.58\n",
      "Playing episode 750 reward: 17.0 average reward: 11.98\n",
      "Playing episode 755 reward: 12.0 average reward: 11.79\n",
      "Playing episode 760 reward: 13.0 average reward: 11.76\n",
      "Playing episode 765 reward: 12.0 average reward: 11.67\n",
      "Playing episode 770 reward: 14.0 average reward: 11.62\n",
      "Playing episode 775 reward: 6.0 average reward: 11.32\n",
      "Playing episode 780 reward: 7.0 average reward: 11.01\n",
      "Playing episode 785 reward: 12.0 average reward: 10.95\n",
      "Playing episode 790 reward: 8.0 average reward: 11.12\n",
      "Playing episode 795 reward: 2.0 average reward: 10.89\n",
      "Playing episode 800 reward: 10.0 average reward: 10.77\n",
      "Playing episode 805 reward: 9.0 average reward: 10.54\n",
      "Playing episode 810 reward: 7.0 average reward: 10.47\n",
      "Playing episode 815 reward: 13.0 average reward: 10.32\n",
      "Playing episode 820 reward: 14.0 average reward: 10.64\n",
      "Playing episode 825 reward: 10.0 average reward: 10.48\n",
      "Playing episode 830 reward: 3.0 average reward: 10.45\n",
      "Playing episode 835 reward: 15.0 average reward: 10.6\n",
      "Playing episode 840 reward: 8.0 average reward: 10.6\n",
      "Playing episode 845 reward: 12.0 average reward: 10.67\n",
      "Playing episode 850 reward: 14.0 average reward: 10.51\n",
      "Playing episode 855 reward: 18.0 average reward: 10.76\n",
      "Playing episode 860 reward: 12.0 average reward: 10.76\n",
      "Playing episode 865 reward: 15.0 average reward: 10.77\n",
      "Playing episode 870 reward: 17.0 average reward: 10.9\n",
      "Playing episode 875 reward: 6.0 average reward: 11.0\n",
      "Playing episode 880 reward: 18.0 average reward: 11.45\n",
      "Playing episode 885 reward: 6.0 average reward: 11.37\n",
      "Playing episode 890 reward: 16.0 average reward: 11.36\n",
      "Playing episode 895 reward: 4.0 average reward: 11.48\n",
      "Playing episode 900 reward: 7.0 average reward: 11.48\n",
      "Playing episode 905 reward: 10.0 average reward: 11.73\n",
      "Playing episode 910 reward: 16.0 average reward: 12.05\n",
      "Playing episode 915 reward: 9.0 average reward: 11.97\n",
      "Playing episode 920 reward: 10.0 average reward: 11.6\n",
      "Playing episode 925 reward: 13.0 average reward: 11.83\n",
      "Playing episode 930 reward: 20.0 average reward: 12.09\n",
      "Playing episode 935 reward: 13.0 average reward: 12.15\n",
      "Playing episode 940 reward: 14.0 average reward: 12.21\n",
      "Playing episode 945 reward: 1.0 average reward: 12.19\n",
      "Playing episode 950 reward: 7.0 average reward: 12.13\n",
      "Playing episode 955 reward: 18.0 average reward: 12.07\n",
      "Playing episode 960 reward: 9.0 average reward: 11.99\n",
      "Playing episode 965 reward: 12.0 average reward: 12.12\n",
      "Playing episode 970 reward: 15.0 average reward: 12.28\n",
      "Playing episode 975 reward: 12.0 average reward: 12.36\n",
      "Playing episode 980 reward: 7.0 average reward: 12.11\n",
      "Playing episode 985 reward: 4.0 average reward: 12.1\n",
      "Playing episode 990 reward: 13.0 average reward: 12.05\n",
      "Playing episode 995 reward: 12.0 average reward: 11.89\n",
      "Problem solved after 999 episodes, average reward: 11.89\n",
      "Setting\n",
      "Playing episode 0 reward: 2.0 average reward: 2.0\n",
      "Playing episode 5 reward: 1.0 average reward: 0.0\n",
      "Playing episode 10 reward: 5.0 average reward: 1.0\n",
      "Playing episode 15 reward: 4.0 average reward: 2.0625\n",
      "Playing episode 20 reward: 8.0 average reward: 3.04761904762\n",
      "Playing episode 25 reward: 4.0 average reward: 3.26923076923\n",
      "Playing episode 30 reward: 0.0 average reward: 3.16129032258\n",
      "Playing episode 35 reward: 1.0 average reward: 3.30555555556\n",
      "Playing episode 40 reward: 4.0 average reward: 3.36585365854\n",
      "Playing episode 45 reward: 8.0 average reward: 3.52173913043\n",
      "Playing episode 50 reward: 15.0 average reward: 3.80392156863\n",
      "Playing episode 55 reward: 1.0 average reward: 4.07142857143\n",
      "Playing episode 60 reward: 15.0 average reward: 4.59016393443\n",
      "Playing episode 65 reward: 3.0 average reward: 4.84848484848\n",
      "Playing episode 70 reward: 10.0 average reward: 5.11267605634\n",
      "Playing episode 75 reward: 4.0 average reward: 5.30263157895\n",
      "Playing episode 80 reward: 14.0 average reward: 5.79012345679\n",
      "Playing episode 85 reward: 0.0 average reward: 6.09302325581\n",
      "Playing episode 90 reward: 14.0 average reward: 6.3956043956\n",
      "Playing episode 95 reward: 9.0 average reward: 6.64583333333\n",
      "Playing episode 100 reward: 15.0 average reward: 6.95\n",
      "Playing episode 105 reward: 0.0 average reward: 7.36\n",
      "Playing episode 110 reward: 15.0 average reward: 7.92\n",
      "Playing episode 115 reward: 10.0 average reward: 8.27\n",
      "Playing episode 120 reward: 7.0 average reward: 8.54\n",
      "Playing episode 125 reward: 14.0 average reward: 8.97\n",
      "Playing episode 130 reward: 10.0 average reward: 9.52\n",
      "Playing episode 135 reward: 17.0 average reward: 9.88\n",
      "Playing episode 140 reward: 16.0 average reward: 10.3\n",
      "Playing episode 145 reward: 11.0 average reward: 10.57\n",
      "Playing episode 150 reward: 11.0 average reward: 10.82\n",
      "Playing episode 155 reward: 14.0 average reward: 11.25\n",
      "Playing episode 160 reward: 16.0 average reward: 11.37\n",
      "Playing episode 165 reward: 15.0 average reward: 11.67\n",
      "Playing episode 170 reward: 15.0 average reward: 11.9\n",
      "Playing episode 175 reward: 12.0 average reward: 12.2\n",
      "Playing episode 180 reward: 4.0 average reward: 11.95\n",
      "Playing episode 185 reward: 13.0 average reward: 11.97\n",
      "Playing episode 190 reward: 13.0 average reward: 11.91\n",
      "Playing episode 195 reward: 11.0 average reward: 12.24\n",
      "Playing episode 200 reward: 9.0 average reward: 12.12\n",
      "Playing episode 205 reward: 7.0 average reward: 12.41\n",
      "Playing episode 210 reward: 11.0 average reward: 12.16\n",
      "Playing episode 215 reward: 17.0 average reward: 12.2\n",
      "Playing episode 220 reward: 13.0 average reward: 12.31\n",
      "Playing episode 225 reward: 5.0 average reward: 12.23\n",
      "Playing episode 230 reward: 18.0 average reward: 12.06\n",
      "Playing episode 235 reward: 14.0 average reward: 12.11\n",
      "Playing episode 240 reward: 7.0 average reward: 12.16\n",
      "Playing episode 245 reward: 14.0 average reward: 12.37\n",
      "Playing episode 250 reward: 10.0 average reward: 12.45\n",
      "Playing episode 255 reward: 11.0 average reward: 12.25\n",
      "Playing episode 260 reward: 20.0 average reward: 12.36\n",
      "Playing episode 265 reward: 22.0 average reward: 12.47\n",
      "Playing episode 270 reward: 19.0 average reward: 12.46\n",
      "Playing episode 275 reward: 15.0 average reward: 12.48\n",
      "Playing episode 280 reward: 18.0 average reward: 12.63\n",
      "Playing episode 285 reward: 14.0 average reward: 12.61\n",
      "Playing episode 290 reward: 14.0 average reward: 12.75\n",
      "Playing episode 295 reward: 3.0 average reward: 12.44\n",
      "Playing episode 300 reward: 11.0 average reward: 12.56\n",
      "Playing episode 305 reward: 16.0 average reward: 12.57\n",
      "Playing episode 310 reward: 13.0 average reward: 12.78\n",
      "Playing episode 315 reward: 15.0 average reward: 12.77\n",
      "Playing episode 320 reward: 11.0 average reward: 12.82\n",
      "Playing episode 325 reward: 17.0 average reward: 12.99\n",
      "Playing episode 330 reward: 15.0 average reward: 13.19\n",
      "Playing episode 335 reward: 12.0 average reward: 13.14\n",
      "Playing episode 340 reward: 10.0 average reward: 13.11\n",
      "Playing episode 345 reward: 18.0 average reward: 13.05\n",
      "Playing episode 350 reward: 2.0 average reward: 12.77\n",
      "Playing episode 355 reward: 14.0 average reward: 12.86\n",
      "Playing episode 360 reward: 16.0 average reward: 12.68\n",
      "Playing episode 365 reward: 15.0 average reward: 12.59\n",
      "Playing episode 370 reward: 10.0 average reward: 12.71\n",
      "Playing episode 375 reward: 16.0 average reward: 12.79\n",
      "Playing episode 380 reward: 3.0 average reward: 12.91\n",
      "Playing episode 385 reward: 16.0 average reward: 13.03\n",
      "Playing episode 390 reward: 20.0 average reward: 13.09\n",
      "Playing episode 395 reward: 18.0 average reward: 13.4\n",
      "Playing episode 400 reward: 14.0 average reward: 13.56\n",
      "Playing episode 405 reward: 11.0 average reward: 13.59\n",
      "Playing episode 410 reward: 17.0 average reward: 13.54\n",
      "Playing episode 415 reward: 17.0 average reward: 13.75\n",
      "Playing episode 420 reward: 15.0 average reward: 13.76\n",
      "Playing episode 425 reward: 14.0 average reward: 13.76\n",
      "Playing episode 430 reward: 17.0 average reward: 13.71\n",
      "Playing episode 435 reward: 23.0 average reward: 13.94\n",
      "Playing episode 440 reward: 21.0 average reward: 14.18\n",
      "Playing episode 445 reward: 10.0 average reward: 14.17\n",
      "Playing episode 450 reward: 13.0 average reward: 14.62\n",
      "Playing episode 455 reward: 19.0 average reward: 14.84\n",
      "Playing episode 460 reward: 11.0 average reward: 14.93\n",
      "Playing episode 465 reward: 18.0 average reward: 14.8\n",
      "Playing episode 470 reward: 8.0 average reward: 14.69\n",
      "Playing episode 475 reward: 16.0 average reward: 14.66\n",
      "Playing episode 480 reward: 11.0 average reward: 14.55\n",
      "Playing episode 485 reward: 12.0 average reward: 14.51\n",
      "Playing episode 490 reward: 13.0 average reward: 14.46\n",
      "Playing episode 495 reward: 13.0 average reward: 14.19\n",
      "Playing episode 500 reward: 4.0 average reward: 14.01\n",
      "Playing episode 505 reward: 12.0 average reward: 13.77\n",
      "Playing episode 510 reward: 18.0 average reward: 14.14\n",
      "Playing episode 515 reward: 10.0 average reward: 13.87\n",
      "Playing episode 520 reward: 12.0 average reward: 13.73\n",
      "Playing episode 525 reward: 10.0 average reward: 13.72\n",
      "Playing episode 530 reward: 17.0 average reward: 13.59\n",
      "Playing episode 535 reward: 13.0 average reward: 13.41\n",
      "Playing episode 540 reward: 9.0 average reward: 13.22\n",
      "Playing episode 545 reward: 19.0 average reward: 13.28\n",
      "Playing episode 550 reward: 14.0 average reward: 13.14\n",
      "Playing episode 555 reward: 11.0 average reward: 12.78\n",
      "Playing episode 560 reward: 15.0 average reward: 12.75\n",
      "Playing episode 565 reward: 14.0 average reward: 12.97\n",
      "Playing episode 570 reward: 14.0 average reward: 12.98\n",
      "Playing episode 575 reward: 24.0 average reward: 13.05\n",
      "Playing episode 580 reward: 19.0 average reward: 13.31\n",
      "Playing episode 585 reward: 11.0 average reward: 13.31\n",
      "Playing episode 590 reward: 9.0 average reward: 13.35\n",
      "Playing episode 595 reward: 19.0 average reward: 13.4\n",
      "Playing episode 600 reward: 14.0 average reward: 13.52\n",
      "Playing episode 605 reward: 13.0 average reward: 13.76\n",
      "Playing episode 610 reward: 14.0 average reward: 13.49\n",
      "Playing episode 615 reward: 15.0 average reward: 13.56\n",
      "Playing episode 620 reward: 13.0 average reward: 13.73\n",
      "Playing episode 625 reward: 16.0 average reward: 13.78\n",
      "Playing episode 630 reward: 13.0 average reward: 14.0\n",
      "Playing episode 635 reward: 11.0 average reward: 14.11\n",
      "Playing episode 640 reward: 18.0 average reward: 14.26\n",
      "Playing episode 645 reward: 17.0 average reward: 14.29\n",
      "Playing episode 650 reward: 16.0 average reward: 14.19\n",
      "Playing episode 655 reward: 16.0 average reward: 14.33\n",
      "Playing episode 660 reward: 14.0 average reward: 14.4\n",
      "Playing episode 665 reward: 20.0 average reward: 14.28\n",
      "Playing episode 670 reward: 19.0 average reward: 14.44\n",
      "Playing episode 675 reward: 12.0 average reward: 14.37\n",
      "Playing episode 680 reward: 12.0 average reward: 14.23\n",
      "Playing episode 685 reward: 17.0 average reward: 14.4\n",
      "Playing episode 690 reward: 17.0 average reward: 14.54\n",
      "Playing episode 695 reward: 13.0 average reward: 14.65\n",
      "Playing episode 700 reward: 10.0 average reward: 14.65\n",
      "Playing episode 705 reward: 20.0 average reward: 14.72\n",
      "Playing episode 710 reward: 18.0 average reward: 14.73\n",
      "Problem solved after 712 episodes, average reward: 15.01\n",
      "Setting\n",
      "Playing episode 0 reward: -3.0 average reward: -3.0\n",
      "Playing episode 5 reward: 0.0 average reward: -0.833333333333\n",
      "Playing episode 10 reward: 0.0 average reward: -0.454545454545\n",
      "Playing episode 15 reward: 1.0 average reward: -0.3125\n",
      "Playing episode 20 reward: 0.0 average reward: -0.238095238095\n",
      "Playing episode 25 reward: 2.0 average reward: -0.115384615385\n",
      "Playing episode 30 reward: 0.0 average reward: -0.0322580645161\n",
      "Playing episode 35 reward: 1.0 average reward: 0.0277777777778\n",
      "Playing episode 40 reward: 1.0 average reward: 0.170731707317\n",
      "Playing episode 45 reward: 0.0 average reward: 0.282608695652\n",
      "Playing episode 50 reward: 6.0 average reward: 0.588235294118\n",
      "Playing episode 55 reward: 3.0 average reward: 0.857142857143\n",
      "Playing episode 60 reward: 5.0 average reward: 0.918032786885\n",
      "Playing episode 65 reward: -1.0 average reward: 1.28787878788\n",
      "Playing episode 70 reward: 10.0 average reward: 1.84507042254\n",
      "Playing episode 75 reward: 9.0 average reward: 2.25\n",
      "Playing episode 80 reward: 7.0 average reward: 2.48148148148\n",
      "Playing episode 85 reward: 7.0 average reward: 2.74418604651\n",
      "Playing episode 90 reward: 12.0 average reward: 2.94505494505\n",
      "Playing episode 95 reward: 11.0 average reward: 3.13541666667\n",
      "Playing episode 100 reward: 8.0 average reward: 3.46\n",
      "Playing episode 105 reward: 12.0 average reward: 3.95\n",
      "Playing episode 110 reward: 12.0 average reward: 4.53\n",
      "Playing episode 115 reward: 12.0 average reward: 5.12\n",
      "Playing episode 120 reward: 13.0 average reward: 5.6\n",
      "Playing episode 125 reward: 4.0 average reward: 6.09\n",
      "Playing episode 130 reward: 6.0 average reward: 6.52\n",
      "Playing episode 135 reward: 13.0 average reward: 7.11\n",
      "Playing episode 140 reward: 17.0 average reward: 7.69\n",
      "Playing episode 145 reward: 5.0 average reward: 8.06\n",
      "Playing episode 150 reward: 17.0 average reward: 8.52\n",
      "Playing episode 155 reward: 12.0 average reward: 9.07\n",
      "Playing episode 160 reward: 12.0 average reward: 9.39\n",
      "Playing episode 165 reward: 14.0 average reward: 9.86\n",
      "Playing episode 170 reward: 15.0 average reward: 9.93\n",
      "Playing episode 175 reward: 19.0 average reward: 10.22\n",
      "Playing episode 180 reward: 19.0 average reward: 10.59\n",
      "Playing episode 185 reward: 16.0 average reward: 10.95\n",
      "Playing episode 190 reward: 11.0 average reward: 11.34\n",
      "Playing episode 195 reward: 10.0 average reward: 11.42\n",
      "Playing episode 200 reward: 13.0 average reward: 11.4\n",
      "Playing episode 205 reward: 10.0 average reward: 11.52\n",
      "Playing episode 210 reward: 20.0 average reward: 11.64\n",
      "Playing episode 215 reward: 16.0 average reward: 11.81\n",
      "Playing episode 220 reward: 21.0 average reward: 12.18\n",
      "Playing episode 225 reward: 16.0 average reward: 12.25\n",
      "Playing episode 230 reward: 16.0 average reward: 12.35\n",
      "Playing episode 235 reward: 17.0 average reward: 12.42\n",
      "Playing episode 240 reward: 20.0 average reward: 12.69\n",
      "Playing episode 245 reward: 14.0 average reward: 12.83\n",
      "Playing episode 250 reward: 8.0 average reward: 13.03\n",
      "Playing episode 255 reward: 16.0 average reward: 13.01\n",
      "Playing episode 260 reward: 5.0 average reward: 13.18\n",
      "Playing episode 265 reward: 15.0 average reward: 13.12\n",
      "Playing episode 270 reward: 2.0 average reward: 13.13\n",
      "Playing episode 275 reward: 13.0 average reward: 13.11\n",
      "Playing episode 280 reward: 19.0 average reward: 13.3\n",
      "Playing episode 285 reward: 12.0 average reward: 13.1\n",
      "Playing episode 290 reward: 16.0 average reward: 13.08\n",
      "Playing episode 295 reward: 13.0 average reward: 13.4\n",
      "Playing episode 300 reward: 15.0 average reward: 13.66\n",
      "Playing episode 305 reward: 12.0 average reward: 13.72\n",
      "Playing episode 310 reward: 5.0 average reward: 13.61\n",
      "Playing episode 315 reward: 21.0 average reward: 13.66\n",
      "Playing episode 320 reward: 14.0 average reward: 13.55\n",
      "Playing episode 325 reward: 11.0 average reward: 13.67\n",
      "Playing episode 330 reward: 3.0 average reward: 13.66\n",
      "Playing episode 335 reward: 21.0 average reward: 13.71\n",
      "Playing episode 340 reward: 7.0 average reward: 13.43\n",
      "Playing episode 345 reward: 6.0 average reward: 13.35\n",
      "Playing episode 350 reward: 15.0 average reward: 13.14\n",
      "Playing episode 355 reward: 7.0 average reward: 12.98\n",
      "Playing episode 360 reward: 11.0 average reward: 13.03\n",
      "Playing episode 365 reward: 12.0 average reward: 12.92\n",
      "Playing episode 370 reward: 7.0 average reward: 13.06\n",
      "Playing episode 375 reward: 12.0 average reward: 13.04\n",
      "Playing episode 380 reward: 13.0 average reward: 12.72\n",
      "Playing episode 385 reward: 11.0 average reward: 12.96\n",
      "Playing episode 390 reward: 6.0 average reward: 12.82\n",
      "Playing episode 395 reward: 14.0 average reward: 12.71\n",
      "Playing episode 400 reward: 6.0 average reward: 12.45\n",
      "Playing episode 405 reward: 19.0 average reward: 12.56\n",
      "Playing episode 410 reward: 13.0 average reward: 12.46\n",
      "Playing episode 415 reward: 9.0 average reward: 12.28\n",
      "Playing episode 420 reward: 6.0 average reward: 12.06\n",
      "Playing episode 425 reward: 11.0 average reward: 12.09\n",
      "Playing episode 430 reward: 14.0 average reward: 12.23\n",
      "Playing episode 435 reward: 10.0 average reward: 12.16\n",
      "Playing episode 440 reward: 5.0 average reward: 12.11\n",
      "Playing episode 445 reward: 20.0 average reward: 12.27\n",
      "Playing episode 450 reward: 15.0 average reward: 12.39\n",
      "Playing episode 455 reward: 15.0 average reward: 12.52\n",
      "Playing episode 460 reward: 19.0 average reward: 12.6\n",
      "Playing episode 465 reward: 8.0 average reward: 12.67\n",
      "Playing episode 470 reward: 11.0 average reward: 12.61\n",
      "Playing episode 475 reward: 13.0 average reward: 12.65\n",
      "Playing episode 480 reward: 18.0 average reward: 12.83\n",
      "Playing episode 485 reward: 13.0 average reward: 12.69\n",
      "Playing episode 490 reward: 9.0 average reward: 12.72\n",
      "Playing episode 495 reward: 21.0 average reward: 12.8\n",
      "Playing episode 500 reward: 12.0 average reward: 12.99\n",
      "Playing episode 505 reward: 15.0 average reward: 12.87\n",
      "Playing episode 510 reward: 19.0 average reward: 13.09\n",
      "Playing episode 515 reward: 5.0 average reward: 12.98\n",
      "Playing episode 520 reward: 6.0 average reward: 13.09\n",
      "Playing episode 525 reward: 15.0 average reward: 12.99\n",
      "Playing episode 530 reward: 3.0 average reward: 12.82\n",
      "Playing episode 535 reward: 12.0 average reward: 12.9\n",
      "Playing episode 540 reward: 11.0 average reward: 12.98\n",
      "Playing episode 545 reward: 13.0 average reward: 12.9\n",
      "Playing episode 550 reward: 15.0 average reward: 12.82\n",
      "Playing episode 555 reward: 5.0 average reward: 12.66\n",
      "Playing episode 560 reward: 12.0 average reward: 12.7\n",
      "Playing episode 565 reward: 7.0 average reward: 12.75\n",
      "Playing episode 570 reward: 17.0 average reward: 12.69\n",
      "Playing episode 575 reward: 6.0 average reward: 12.42\n",
      "Playing episode 580 reward: 15.0 average reward: 12.36\n",
      "Playing episode 585 reward: 21.0 average reward: 12.6\n",
      "Playing episode 590 reward: 12.0 average reward: 12.53\n",
      "Playing episode 595 reward: 9.0 average reward: 12.43\n",
      "Playing episode 600 reward: 9.0 average reward: 12.45\n",
      "Playing episode 605 reward: 12.0 average reward: 12.44\n",
      "Playing episode 610 reward: 8.0 average reward: 12.43\n",
      "Playing episode 615 reward: 14.0 average reward: 12.43\n",
      "Playing episode 620 reward: 9.0 average reward: 12.28\n",
      "Playing episode 625 reward: 9.0 average reward: 12.22\n",
      "Playing episode 630 reward: 12.0 average reward: 12.41\n",
      "Playing episode 635 reward: 5.0 average reward: 12.27\n",
      "Playing episode 640 reward: 11.0 average reward: 12.33\n",
      "Playing episode 645 reward: 2.0 average reward: 12.36\n",
      "Playing episode 650 reward: 10.0 average reward: 12.43\n",
      "Playing episode 655 reward: 9.0 average reward: 12.44\n",
      "Playing episode 660 reward: 22.0 average reward: 12.54\n",
      "Playing episode 665 reward: 18.0 average reward: 12.46\n",
      "Playing episode 670 reward: 5.0 average reward: 12.4\n",
      "Playing episode 675 reward: 13.0 average reward: 12.62\n",
      "Playing episode 680 reward: 9.0 average reward: 12.57\n",
      "Playing episode 685 reward: 11.0 average reward: 12.31\n",
      "Playing episode 690 reward: 17.0 average reward: 12.51\n",
      "Playing episode 695 reward: 12.0 average reward: 12.5\n",
      "Playing episode 700 reward: 13.0 average reward: 12.45\n",
      "Playing episode 705 reward: 10.0 average reward: 12.3\n",
      "Playing episode 710 reward: 9.0 average reward: 12.07\n",
      "Playing episode 715 reward: 12.0 average reward: 12.09\n",
      "Playing episode 720 reward: 14.0 average reward: 12.2\n",
      "Playing episode 725 reward: 6.0 average reward: 12.14\n",
      "Playing episode 730 reward: 15.0 average reward: 12.01\n",
      "Playing episode 735 reward: 17.0 average reward: 12.13\n",
      "Playing episode 740 reward: 10.0 average reward: 12.1\n",
      "Playing episode 745 reward: 19.0 average reward: 12.25\n",
      "Playing episode 750 reward: 11.0 average reward: 12.24\n",
      "Playing episode 755 reward: 12.0 average reward: 12.36\n",
      "Playing episode 760 reward: 19.0 average reward: 12.32\n",
      "Playing episode 765 reward: 9.0 average reward: 12.36\n",
      "Playing episode 770 reward: 18.0 average reward: 12.36\n",
      "Playing episode 775 reward: 19.0 average reward: 12.48\n",
      "Playing episode 780 reward: 17.0 average reward: 12.7\n",
      "Playing episode 785 reward: 18.0 average reward: 12.81\n",
      "Playing episode 790 reward: 14.0 average reward: 12.8\n",
      "Playing episode 795 reward: 18.0 average reward: 12.89\n",
      "Playing episode 800 reward: 14.0 average reward: 12.89\n",
      "Playing episode 805 reward: 16.0 average reward: 13.12\n",
      "Playing episode 810 reward: 16.0 average reward: 13.41\n",
      "Playing episode 815 reward: 17.0 average reward: 13.73\n",
      "Playing episode 820 reward: 17.0 average reward: 13.84\n",
      "Playing episode 825 reward: 18.0 average reward: 13.91\n",
      "Playing episode 830 reward: 11.0 average reward: 14.04\n",
      "Playing episode 835 reward: 16.0 average reward: 13.97\n",
      "Playing episode 840 reward: 7.0 average reward: 13.93\n",
      "Playing episode 845 reward: 6.0 average reward: 13.86\n",
      "Playing episode 850 reward: 18.0 average reward: 13.76\n",
      "Playing episode 855 reward: 10.0 average reward: 13.69\n",
      "Playing episode 860 reward: 9.0 average reward: 13.4\n",
      "Playing episode 865 reward: 18.0 average reward: 13.52\n",
      "Playing episode 870 reward: 7.0 average reward: 13.6\n",
      "Playing episode 875 reward: 11.0 average reward: 13.64\n",
      "Playing episode 880 reward: 13.0 average reward: 13.48\n",
      "Playing episode 885 reward: 6.0 average reward: 13.18\n",
      "Playing episode 890 reward: 18.0 average reward: 13.2\n",
      "Playing episode 895 reward: 11.0 average reward: 13.13\n",
      "Playing episode 900 reward: 19.0 average reward: 13.39\n",
      "Playing episode 905 reward: 19.0 average reward: 13.45\n",
      "Playing episode 910 reward: 17.0 average reward: 13.35\n",
      "Playing episode 915 reward: 15.0 average reward: 13.23\n",
      "Playing episode 920 reward: 17.0 average reward: 13.28\n",
      "Playing episode 925 reward: 17.0 average reward: 13.52\n",
      "Playing episode 930 reward: 15.0 average reward: 13.53\n",
      "Playing episode 935 reward: 2.0 average reward: 13.5\n",
      "Playing episode 940 reward: 0.0 average reward: 13.38\n",
      "Playing episode 945 reward: 17.0 average reward: 13.42\n",
      "Playing episode 950 reward: 16.0 average reward: 13.49\n",
      "Playing episode 955 reward: 18.0 average reward: 13.65\n",
      "Playing episode 960 reward: 11.0 average reward: 13.79\n",
      "Playing episode 965 reward: 11.0 average reward: 13.65\n",
      "Playing episode 970 reward: 14.0 average reward: 13.85\n",
      "Playing episode 975 reward: 9.0 average reward: 13.57\n",
      "Playing episode 980 reward: 8.0 average reward: 13.49\n",
      "Playing episode 985 reward: 20.0 average reward: 13.87\n",
      "Playing episode 990 reward: 7.0 average reward: 13.72\n",
      "Playing episode 995 reward: 9.0 average reward: 13.64\n",
      "Problem solved after 999 episodes, average reward: 13.52\n"
     ]
    }
   ],
   "source": [
    "# Loading environments & setting brain, as in Navigation.ipynb from [6]:\n",
    "env = UnityEnvironment(file_name=environment_path)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "state = env_info.vector_observations[0] \n",
    "\n",
    "action_size = brain.vector_action_space_size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "#\n",
    "\n",
    "result_collector = []\n",
    "for i, setting in enumerate(test_settings):\n",
    "    \n",
    "    print(\"Setting\")\n",
    "    # construct the agent\n",
    "    agent = Agent(state_size, action_size, setting)\n",
    "\n",
    "    # reset the lists for monitoring progress\n",
    "    all_rewards = []\n",
    "    avg_rewards = []\n",
    "    last_rewards = deque(maxlen = 100)\n",
    "    \n",
    "    # start training\n",
    "    for n in range(number_episodes):\n",
    "        sum_reward = 0\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0] \n",
    "        while True:\n",
    "            action = agent.get_action(state)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            experience = Experience(state, action, reward, next_state, done)\n",
    "            agent.update(experience)\n",
    "            sum_reward += reward     \n",
    "            state = next_state                             \n",
    "            if done:                                       \n",
    "                break\n",
    "        all_rewards.append(sum_reward)\n",
    "        last_rewards.append(sum_reward)\n",
    "        avg = np.mean(last_rewards)\n",
    "        avg_rewards.append(avg)\n",
    "        if( avg > 15 or n == number_episodes - 1):\n",
    "            print(\"Problem solved after \" + str(n) + \" episodes, average reward: \" + str(avg))\n",
    "            result_collector.append(avg_rewards)\n",
    "            env.reset(train_mode=True)[brain_name]\n",
    "            agent.save_nns(model_save_path)\n",
    "            break\n",
    "        if n%5 == 0:\n",
    "            print(\"Playing episode \" + str(n) + \" reward: \" + str(np.mean(sum_reward)) + \" average reward: \" + str(np.mean(last_rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "While the original goal was to have an agent that achieves an average score above 13 in 100 consecutives episodes.  However, since all three agents achieve this goal rather quickly, I set up the goal to 15 in order to see what performance the agents obtain in the longrun. Below is a figure of the average rewards for each of the three different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e9ce55dd8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [setting.name for setting in test_settings]\n",
    "for name, result in zip(labels, result_collector):\n",
    "    plt.plot(result, label = name)\n",
    "plt.xlabel('Number of Episodes')\n",
    "plt.ylabel('Average reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "![figure.png](attachment:figure.png)\n",
    "The figure above shows a result form an earlier run. The DDQN network seems to converge to a good solution quicker than the two other methods and has lower variance after reaching the plateau. One possible explanation for the bad performance of the DDQN + Duel method might be that the layout of the neural network is not ideal for this task. Additionally, the \"Rainbow \" manuscript by Hessel et al. (2017, [8]), who compared the performance of various DQN methods on 57 diffrent Atari games, showed that not all improvements lead to a better performance on each individual task. \n",
    "\n",
    "For the future implementing the different improvements from [8] into the agent would be an interesting direction. For example, Prioritized Experience Replay [9] focuses on using updates that are good candidates to improve the TD-error rather than randomly sampling updates from the replay memory. However, even without further improvements the performance is already quite good on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Juliani, A., Berges, V., Vckay, E., Gao, Y., Henry, H., Mattar, M., Lange, D. (2018). Unity: A General Platform for Intelligent Agents. arXiv preprint arXiv:1809.02627. https://github.com/Unity-Technologies/ml-agents.\n",
    "\n",
    "[2] Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540 (2015): 529.\n",
    "\n",
    "[3] van Hasselt, Hado, Arthur Guez, and David Silver. \"Deep Reinforcement Learning with Double Q-learning.\" arXiv preprint arXiv:1509.06461v3 (2015).\n",
    "\n",
    "[4] Wang, Ziyu, et al. \"Dueling network architectures for deep reinforcement learning.\" arXiv preprint arXiv:1511.06581v3 (2015).\n",
    "\n",
    "[5] https://github.com/udacity/deep-reinforcement-learning, last accessed: 10.03.2019\n",
    "\n",
    "[6] https://github.com/udacity/deep-reinforcement-learning/tree/master/p1_navigation, last accessed: 10.03.2019\n",
    "\n",
    "[7] https://github.com/openai/baselines, last accessed: 10.03.2019\n",
    "\n",
    "[8] https://pytorch.org/tutorials/, last accessed: 10.03.2019\n",
    "\n",
    "[9] Hessel, Matteo, et al. \"Rainbow: Combining Improvements in Deep Reinforcement Learning.\" arXiv preprint arXiv:1710.02298 (2017).\n",
    "\n",
    "[10] Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint arXiv:1511.05952 (2015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
